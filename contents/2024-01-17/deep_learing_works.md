---
date: '2024-01-17'
title: '딥러닝 용어필기'
categories: ['DataScience']
summary: '공부하면서 용어를 정리해 봤습니다.'
thumbnail: '../../static/default.png'
---

- **w(weight, 가중치)** :<br>
  입력층 데이터를 다음층으로 넘길 때의 중요도 뿐만 아니라, 신경망 전체의 성능에 큰 영향을 미치는 매우 중요한 요소입니다.

- **learning rate(학습률)** :<br>
  신경망이 가중치의 값을 얼마나 변화시킬지 학습하는 속도를 나타냅니다.

- **epoch** :<br>
  오차를 줄이기 위해 계산을 몇 번 수행해야 하는지를 나타내는 값입니다. 5 에포크일 때, 신경망은 전체 데이터셋을 5번 학습합니다. 각 에포크가 끝날 때마다 가중치가 업데이트됩니다.

- **loss function** :<br>
  실제 값과 예측 값의 차이를 계산하는 함수입니다. 일반적으로 오차를 계산하는데 사용됩니다.

- **step function(계단함수)** :<br>
  1 또는 0의 값을 가지는 함수입니다. 입력 값이 임계치(threshold)보다 작으면 0, 크면 1을 출력합니다.

- **신경망 알고리즘** :<br>
  1*가중치 무작위 초기화 2*출력값 계산 3*예측값 계산 하여 오류 계산 4*새로운 가중치를 찾는 계산 5*가중치 업데이트 6*설정 epoch만큼 반복하여 학습 종료

- **경사하강법** :<br>
  오류가 가장 적은 지점으로 가중치를 업데이트합니다. 기울기(gradient)를 계산하기 위해 편미분(partial derivative)을 사용합니다. 이때, 시그모이드 함수의 미분값(도함수)은 y(1 - y)입니다.

- **가중치 조정 계산** :<br>
  1*시그모이드 함수 활성 2*편미분 계산 3\_델타 매개 변수 계산(그래디언트 계산 보조 변수=>방향결정)

- **delta_output(델타 매개 변수)** :<br>
  error \* sigmoid_derivative(y)

- **Batch gradient descent(배치경사하강법)** :<br>
  전체 데이터셋에 대한 오류를 계산한 후 가중치를 업데이트합니다.

- **Stochastic gradient descent(확률적 경사하강법)** :<br>
  각 데이터 인스턴스에 대한 오류를 계산한 다음 가중치를 업데이트합니다.
  지역 최솟값 방지 = 가중치 업데이트를 위해 전체 업데이트를 선택하지 않았기 때문.
  빠르다 = 전체 메모리에 업데이트 할 필요 없기 때문.

- **Mini-batch gradient descent** :<br>
  batch gradient descent와 stochastic gradient descent의 장점을 결합한 알고리즘으로,
  데이터를 작은 배치로 분할하여 기울기를 계산합니다. 이렇게 함으로써 오류 계산의 정확성은 batch gradient descent에 가깝고,
  계산 시간은 stochastic gradient descent에 가깝게 유지됩니다.
  일반적으로 mini-batch의 크기는 32, 64, 128 등의 작은 값으로 지정됩니다. 이 크기는 하이퍼파라미터로서 조정이 가능하며, 작은 값일수록 더 빠른 계산이 가능하지만, 불안정한 기울기를 계산할 가능성이 있습니다.
